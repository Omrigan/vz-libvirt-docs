<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="Docutils 0.16: http://docutils.sourceforge.net/" />
<title>KVM Real Time Guest Config</title>

</head>
<body>
<div class="document" id="kvm-real-time-guest-config">
<h1 class="title">KVM Real Time Guest Config</h1>

<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#host-partitioning-plan" id="id1">Host partitioning plan</a></p></li>
<li><p><a class="reference internal" href="#guest-configuration" id="id2">Guest configuration</a></p>
<ul>
<li><p><a class="reference internal" href="#cpu-configuration" id="id3">CPU configuration</a></p></li>
<li><p><a class="reference internal" href="#memory-configuration" id="id4">Memory configuration</a></p></li>
<li><p><a class="reference internal" href="#device-configuration" id="id5">Device configuration</a></p></li>
</ul>
</li>
</ul>
</div>
<p>The KVM hypervisor is capable of running real time guest workloads. This page
describes the key pieces of configuration required in the domain XML to achieve
the low latency needs of real time workloads.</p>
<p>For the most part, configuration of the host OS is out of scope of this
documentation. Refer to the operating system vendor's guidance on configuring
the host OS and hardware for real time. Note in particular that the default
kernel used by most Linux distros is not suitable for low latency real time and
must be replaced by a special kernel build.</p>
<div class="section" id="host-partitioning-plan">
<h1><a class="toc-backref" href="#id1">Host partitioning plan</a></h1>
<p>Running real time workloads requires carefully partitioning up the host OS
resources, such that the KVM / QEMU processes are strictly separated from any
other workload running on the host, both userspace processes and kernel threads.</p>
<p>As such, some subset of host CPUs need to be reserved exclusively for running
KVM guests. This requires that the host kernel be booted using the <span class="docutils literal">isolcpus</span>
kernel command line parameter. This parameter removes a set of CPUs from the
scheduler, such that that no kernel threads or userspace processes will ever get
placed on those CPUs automatically. KVM guests are then manually placed onto
these CPUs.</p>
<p>Deciding which host CPUs to reserve for real time requires understanding of the
guest workload needs and balancing with the host OS needs. The trade off will
also vary based on the physical hardware available.</p>
<p>For the sake of illustration, this guide will assume a physical machine with two
NUMA nodes, each with 2 sockets and 4 cores per socket, giving a total of 16
CPUs on the host. Furthermore, it is assumed that hyperthreading is either not
supported or has been disabled in the BIOS, since it is incompatible with real
time. Each NUMA node is assumed to have 32 GB of RAM, giving 64 GB total for
the host.</p>
<p>It is assumed that 2 CPUs in each NUMA node are reserved for the host OS, with
the remaining 6 CPUs available for KVM real time. With this in mind, the host
kernel should have booted with <span class="docutils literal"><span class="pre">isolcpus=2-7,10-15</span></span> to reserve CPUs.</p>
<p>To maximise efficiency of page table lookups for the guest, the host needs to be
configured with most RAM exposed as huge pages, ideally 1 GB sized. 6 GB of RAM
in each NUMA node will be reserved for general host OS usage as normal sized
pages, leaving 26 GB for KVM usage as huge pages.</p>
<p>Once huge pages are reserved on the hypothetical machine, the <span class="docutils literal">virsh capabilities</span> command output is expected to look approximately like:</p>
<pre class="literal-block">&lt;topology&gt;
  &lt;cells num='2'&gt;
    &lt;cell id='0'&gt;
      &lt;memory unit='KiB'&gt;33554432&lt;/memory&gt;
      &lt;pages unit='KiB' size='4'&gt;1572864&lt;/pages&gt;
      &lt;pages unit='KiB' size='2048'&gt;0&lt;/pages&gt;
      &lt;pages unit='KiB' size='1048576'&gt;26&lt;/pages&gt;
      &lt;distances&gt;
        &lt;sibling id='0' value='10'/&gt;
        &lt;sibling id='1' value='21'/&gt;
      &lt;/distances&gt;
      &lt;cpus num='8'&gt;
        &lt;cpu id='0' socket_id='0' core_id='0' siblings='0'/&gt;
        &lt;cpu id='1' socket_id='0' core_id='1' siblings='1'/&gt;
        &lt;cpu id='2' socket_id='0' core_id='2' siblings='2'/&gt;
        &lt;cpu id='3' socket_id='0' core_id='3' siblings='3'/&gt;
        &lt;cpu id='4' socket_id='1' core_id='0' siblings='4'/&gt;
        &lt;cpu id='5' socket_id='1' core_id='1' siblings='5'/&gt;
        &lt;cpu id='6' socket_id='1' core_id='2' siblings='6'/&gt;
        &lt;cpu id='7' socket_id='1' core_id='3' siblings='7'/&gt;
      &lt;/cpus&gt;
    &lt;/cell&gt;
    &lt;cell id='1'&gt;
      &lt;memory unit='KiB'&gt;33554432&lt;/memory&gt;
      &lt;pages unit='KiB' size='4'&gt;1572864&lt;/pages&gt;
      &lt;pages unit='KiB' size='2048'&gt;0&lt;/pages&gt;
      &lt;pages unit='KiB' size='1048576'&gt;26&lt;/pages&gt;
      &lt;distances&gt;
        &lt;sibling id='0' value='21'/&gt;
        &lt;sibling id='1' value='10'/&gt;
      &lt;/distances&gt;
      &lt;cpus num='8'&gt;
        &lt;cpu id='8' socket_id='0' core_id='0' siblings='8'/&gt;
        &lt;cpu id='9' socket_id='0' core_id='1' siblings='9'/&gt;
        &lt;cpu id='10' socket_id='0' core_id='2' siblings='10'/&gt;
        &lt;cpu id='11' socket_id='0' core_id='3' siblings='11'/&gt;
        &lt;cpu id='12' socket_id='1' core_id='0' siblings='12'/&gt;
        &lt;cpu id='13' socket_id='1' core_id='1' siblings='13'/&gt;
        &lt;cpu id='14' socket_id='1' core_id='2' siblings='14'/&gt;
        &lt;cpu id='15' socket_id='1' core_id='3' siblings='15'/&gt;
       &lt;/cpus&gt;
    &lt;/cell&gt;
  &lt;/cells&gt;
&lt;/topology&gt;</pre>
<p>Be aware that CPU ID numbers are not always allocated sequentially as shown
here. It is not unusual to see IDs interleaved between sockets on the two NUMA
nodes, such that <span class="docutils literal"><span class="pre">0-3,8-11</span></span> are on the first node and <span class="docutils literal"><span class="pre">4-7,12-15</span></span> are on
the second node.  Carefully check the <span class="docutils literal">virsh capabilities</span> output to determine
the CPU ID numbers when configiring both <span class="docutils literal">isolcpus</span> and the guest <span class="docutils literal">cpuset</span>
values.</p>
</div>
<div class="section" id="guest-configuration">
<h1><a class="toc-backref" href="#id2">Guest configuration</a></h1>
<p>What follows is an overview of the key parts of the domain XML that need to be
configured to achieve low latency for real time workflows. The following example
will assume a 4 CPU guest, requiring 16 GB of RAM. It is intended to be placed
on the second host NUMA node.</p>
<div class="section" id="cpu-configuration">
<h2><a class="toc-backref" href="#id3">CPU configuration</a></h2>
<p>Real time KVM guests intended to run Linux should have a minimum of 2 CPUs.
One vCPU is for running non-real time processes and performing I/O. The other
vCPUs will run real time applications. Some non-Linux OS may not require a
special non-real time CPU to be available, in which case the 2 CPU minimum would
not apply.</p>
<p>Each guest CPU, even the non-real time one, needs to be pinned to a dedicated
host core that is in the <cite>isolcpus</cite> reserved set. The QEMU emulator threads
need to be pinned to host CPUs that are not in the <cite>isolcpus</cite> reserved set.
The vCPUs need to be given a real time CPU scheduler policy.</p>
<p>When configuring the <a class="reference external" href="../formatdomain.html#cpu-allocation">guest CPU count</a>,
do not include any CPU affinity at this stage:</p>
<pre class="literal-block">&lt;vcpu placement='static'&gt;4&lt;/vcpu&gt;</pre>
<p>The guest CPUs now need to be placed individually. In this case, they will all
be put within the same host socket, such that they can be exposed as core
siblings. This is achieved using the <a class="reference external" href="../formatdomain.html#cpu-tuning">CPU tuning config</a>:</p>
<pre class="literal-block">&lt;cputune&gt;
  &lt;emulatorpin cpuset=&quot;8-9&quot;/&gt;
  &lt;vcpupin vcpu=&quot;0&quot; cpuset=&quot;12&quot;/&gt;
  &lt;vcpupin vcpu=&quot;1&quot; cpuset=&quot;13&quot;/&gt;
  &lt;vcpupin vcpu=&quot;2&quot; cpuset=&quot;14&quot;/&gt;
  &lt;vcpupin vcpu=&quot;3&quot; cpuset=&quot;15&quot;/&gt;
  &lt;vcpusched vcpus='0-4' scheduler='fifo' priority='1'/&gt;
&lt;/cputune&gt;</pre>
<p>The <a class="reference external" href="../formatdomain.html#cpu-model-and-topology">guest CPU model</a> now needs to be
configured to pass through the host model unchanged, with topology matching the
placement:</p>
<pre class="literal-block">&lt;cpu mode='host-passthrough'&gt;
  &lt;topology sockets='1' dies='1' cores='4' threads='1'/&gt;
  &lt;feature policy='require' name='tsc-deadline'/&gt;
&lt;/cpu&gt;</pre>
<p>The performance monitoring unit virtualization needs to be disabled
via the <a class="reference external" href="../formatdomain.html#hypervisor-features">hypervisor features</a>:</p>
<pre class="literal-block">&lt;features&gt;
  ...
  &lt;pmu state='off'/&gt;
&lt;/features&gt;</pre>
</div>
<div class="section" id="memory-configuration">
<h2><a class="toc-backref" href="#id4">Memory configuration</a></h2>
<p>The host memory used for guest RAM needs to be allocated from huge pages on the
second NUMA node, and all other memory allocation needs to be locked into RAM
with memory page sharing disabled.
This is achieved by using the <a class="reference external" href="../formatdomain.html#memory-backing">memory backing config</a>:</p>
<pre class="literal-block">&lt;memoryBacking&gt;
  &lt;hugepages&gt;
    &lt;page size=&quot;1&quot; unit=&quot;G&quot; nodeset=&quot;1&quot;/&gt;
  &lt;/hugepages&gt;
  &lt;locked/&gt;
  &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</pre>
</div>
<div class="section" id="device-configuration">
<h2><a class="toc-backref" href="#id5">Device configuration</a></h2>
<p>Libvirt adds a few devices by default to maintain historical QEMU configuration
behaviour. It is unlikely these devices are required by real time guests, so it
is wise to disable them. Remove all USB controllers that may exist in the XML
config and replace them with:</p>
<pre class="literal-block">&lt;controller type=&quot;usb&quot; model=&quot;none&quot;/&gt;</pre>
<p>Similarly the memory balloon config should be changed to</p>
<pre class="literal-block">&lt;memballoon model=&quot;none&quot;/&gt;</pre>
<p>If the guest had a graphical console at installation time this can also be
disabled, with remote access being over SSH, with a minimal serial console
for emergencies.</p>
</div>
</div>
</div>
</body>
</html>
