<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
  <!--
        This file is autogenerated from docs/migration.rst
        Do not edit this file. Changes will be lost.
      -->
  <!--
        This page was generated at Wed Apr 19 07:14:27 2023 UTC.
      -->
  <head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="stylesheet" type="text/css" href="css/main.css"/>
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/>
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/>
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/>
    <link rel="manifest" href="/manifest.json"/>
    <meta name="theme-color" content="#ffffff"/>
    <title>libvirt: Guest migration</title>
    <meta name="description" content="libvirt, virtualization, virtualization API"/>
    <script type="text/javascript" src="js/main.js">
      <!--// forces non-empty element-->
    </script>
  </head>
  <body onload="pageload()">
    <div id="body">
      <div class="document" id="guest-migration">
<h1>Guest migration</h1>

<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#network-data-transports" id="id1">Network data transports</a></p>
<ul>
<li><p><a class="reference internal" href="#hypervisor-native-transport" id="id2">Hypervisor native transport</a></p></li>
<li><p><a class="reference internal" href="#libvirt-tunnelled-transport" id="id3">libvirt tunnelled transport</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#communication-control-paths-flows" id="id4">Communication control paths/flows</a></p>
<ul>
<li><p><a class="reference internal" href="#managed-direct-migration" id="id5">Managed direct migration</a></p></li>
<li><p><a class="reference internal" href="#managed-peer-to-peer-migration" id="id6">Managed peer to peer migration</a></p></li>
<li><p><a class="reference internal" href="#unmanaged-direct-migration" id="id7">Unmanaged direct migration</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#data-security" id="id8">Data security</a></p></li>
<li><p><a class="reference internal" href="#offline-migration" id="id9">Offline migration</a></p></li>
<li><p><a class="reference internal" href="#migration-uris" id="id10">Migration URIs</a></p></li>
<li><p><a class="reference internal" href="#configuration-file-handling" id="id11">Configuration file handling</a></p></li>
<li><p><a class="reference internal" href="#migration-scenarios" id="id12">Migration scenarios</a></p>
<ul>
<li><p><a class="reference internal" href="#native-migration-client-to-two-libvirtd-servers" id="id13">Native migration, client to two libvirtd servers</a></p></li>
<li><p><a class="reference internal" href="#native-migration-client-to-and-peer2peer-between-two-libvirtd-servers" id="id14">Native migration, client to and peer2peer between, two libvirtd servers</a></p></li>
<li><p><a class="reference internal" href="#tunnelled-migration-client-and-peer2peer-between-two-libvirtd-servers" id="id15">Tunnelled migration, client and peer2peer between two libvirtd servers</a></p></li>
<li><p><a class="reference internal" href="#native-migration-client-to-one-libvirtd-server" id="id16">Native migration, client to one libvirtd server</a></p></li>
<li><p><a class="reference internal" href="#native-migration-peer2peer-between-two-libvirtd-servers" id="id17">Native migration, peer2peer between two libvirtd servers</a></p></li>
<li><p><a class="reference internal" href="#tunnelled-migration-peer2peer-between-two-libvirtd-servers" id="id18">Tunnelled migration, peer2peer between two libvirtd servers</a></p></li>
<li><p><a class="reference internal" href="#migration-using-only-unix-sockets" id="id19">Migration using only UNIX sockets</a></p></li>
<li><p><a class="reference internal" href="#migration-of-vms-using-non-shared-images-for-disks" id="id20">Migration of VMs using non-shared images for disks</a></p></li>
</ul>
</li>
</ul>
</div>
<p>Migration of guests between hosts is a complicated problem with many possible
solutions, each with their own positive and negative points. For maximum
flexibility of both hypervisor integration, and administrator deployment,
libvirt implements several options for migration.</p>
<div class="section" id="network-data-transports">
<h1><a class="toc-backref" href="#id1">Network data transports</a><a class="headerlink" href="#network-data-transports" title="Link to this headline">¶</a></h1>
<p>There are two options for the data transport used during migration, either the
hypervisor's own <strong>native</strong> transport, or <strong>tunnelled</strong> over a libvirtd
connection.</p>
<div class="section" id="hypervisor-native-transport">
<h2><a class="toc-backref" href="#id2">Hypervisor native transport</a><a class="headerlink" href="#hypervisor-native-transport" title="Link to this headline">¶</a></h2>
<p><em>Native</em> data transports may or may not support encryption, depending on the
hypervisor in question, but will typically have the lowest computational costs
by minimising the number of data copies involved. The native data transports
will also require extra hypervisor-specific network configuration steps by the
administrator when deploying a host. For some hypervisors, it might be necessary
to open up a large range of ports on the firewall to allow multiple concurrent
migration operations.</p>
<p>Modern hypervisors support TLS for encryption and authentication of the
migration connections which can be enabled using the <span class="docutils literal">VIR_MIGRATE_TLS</span> flag.
The <em>qemu</em> hypervisor driver allows users to force use of TLS via the
<span class="docutils literal">migrate_tls_force</span> knob configured in <span class="docutils literal">/etc/libvirt/qemu.conf</span>.</p>
<p><img alt="Migration native path" class="diagram" src="images/migration-native.png"/></p>
</div>
<div class="section" id="libvirt-tunnelled-transport">
<h2><a class="toc-backref" href="#id3">libvirt tunnelled transport</a><a class="headerlink" href="#libvirt-tunnelled-transport" title="Link to this headline">¶</a></h2>
<p><em>Tunnelled</em> data transports will always be capable of strong encryption since
they are able to leverage the capabilities built in to the libvirt RPC protocol.
The downside of a tunnelled transport, however, is that there will be extra data
copies involved on both the source and destinations hosts as the data is moved
between libvirtd and the hypervisor. This is likely to be a more significant
problem for guests with very large RAM sizes, which dirty memory pages quickly.
On the deployment side, tunnelled transports do not require any extra network
configuration over and above what's already required for general libvirtd
<a class="reference external" href="remote.html">remote access</a>, and there is only need for a single port to be
open on the firewall to support multiple concurrent migration operations.</p>
<p><em>Note:</em> Certain features such as migration of non-shared storage
(<span class="docutils literal">VIR_MIGRATE_NON_SHARED_DISK</span>), the multi-connection migration
(<span class="docutils literal">VIR_MIGRATE_PARALLEL</span>), or post-copy migration (<span class="docutils literal">VIR_MIGRATE_POSTCOPY</span>)
may not be available when using libvirt's tunnelling.</p>
<p><img alt="Migration tunnel path" class="diagram" src="images/migration-tunnel.png"/></p>
</div>
</div>
<div class="section" id="communication-control-paths-flows">
<h1><a class="toc-backref" href="#id4">Communication control paths/flows</a><a class="headerlink" href="#communication-control-paths-flows" title="Link to this headline">¶</a></h1>
<p>Migration of virtual machines requires close co-ordination of the two hosts
involved, as well as the application invoking the migration, which may be on the
source, the destination, or a third host.</p>
<div class="section" id="managed-direct-migration">
<h2><a class="toc-backref" href="#id5">Managed direct migration</a><a class="headerlink" href="#managed-direct-migration" title="Link to this headline">¶</a></h2>
<p>With <em>managed direct</em> migration, the libvirt client process controls the various
phases of migration. The client application must be able to connect and
authenticate with the libvirtd daemons on both the source and destination hosts.
There is no need for the two libvirtd daemons to communicate with each other. If
the client application crashes, or otherwise loses its connection to libvirtd
during the migration process, an attempt will be made to abort the migration and
restart the guest CPUs on the source host. There may be scenarios where this
cannot be safely done, in which cases the guest will be left paused on one or
both of the hosts.</p>
<p><img alt="Migration direct, managed" class="diagram" src="images/migration-managed-direct.png"/></p>
</div>
<div class="section" id="managed-peer-to-peer-migration">
<h2><a class="toc-backref" href="#id6">Managed peer to peer migration</a><a class="headerlink" href="#managed-peer-to-peer-migration" title="Link to this headline">¶</a></h2>
<p>With <em>peer to peer</em> migration, the libvirt client process only talks to the
libvirtd daemon on the source host. The source libvirtd daemon controls the
entire migration process itself, by directly connecting the destination host
libvirtd. If the client application crashes, or otherwise loses its connection
to libvirtd, the migration process will continue uninterrupted until completion.
Note that the source libvirtd uses its own credentials (typically root) to
connect to the destination, rather than the credentials used by the client to
connect to the source; if these differ, it is common to run into a situation
where a client can connect to the destination directly but the source cannot
make the connection to set up the peer-to-peer migration.</p>
<p><img alt="Migration peer-to-peer" class="diagram" src="images/migration-managed-p2p.png"/></p>
</div>
<div class="section" id="unmanaged-direct-migration">
<h2><a class="toc-backref" href="#id7">Unmanaged direct migration</a><a class="headerlink" href="#unmanaged-direct-migration" title="Link to this headline">¶</a></h2>
<p>With <em>unmanaged direct</em> migration, neither the libvirt client or libvirtd daemon
control the migration process. Control is instead delegated to the hypervisor's
over management services (if any). The libvirt client merely initiates the
migration via the hypervisor's management layer. If the libvirt client or
libvirtd crash, the migration process will continue uninterrupted until
completion.</p>
<p><img alt="Migration direct, unmanaged" class="diagram" src="images/migration-unmanaged-direct.png"/></p>
</div>
</div>
<div class="section" id="data-security">
<h1><a class="toc-backref" href="#id8">Data security</a><a class="headerlink" href="#data-security" title="Link to this headline">¶</a></h1>
<p>Since the migration data stream includes a complete copy of the guest OS RAM,
snooping of the migration data stream may allow compromise of sensitive guest
information. If the virtualization hosts have multiple network interfaces, or if
the network switches support tagged VLANs, then it is very desirable to separate
guest network traffic from migration or management traffic.</p>
<p>In some scenarios, even a separate network for migration data may not offer
sufficient security. In this case it is possible to apply encryption to the
migration data stream. If the hypervisor does not itself offer encryption, then
the libvirt tunnelled migration facility should be used.</p>
</div>
<div class="section" id="offline-migration">
<h1><a class="toc-backref" href="#id9">Offline migration</a><a class="headerlink" href="#offline-migration" title="Link to this headline">¶</a></h1>
<p>Offline migration transfers the inactive definition of a domain (which may or
may not be active). After successful completion, the domain remains in its
current state on the source host and is defined but inactive on the destination
host. It's a bit more clever than <span class="docutils literal">virsh dumpxml</span> on source host followed by
<span class="docutils literal">virsh define</span> on destination host, as offline migration will run the
pre-migration hook to update the domain XML on destination host. Currently,
copying non-shared storage or other file based storages (e.g. UEFI variable
storage) is not supported during offline migration.</p>
</div>
<div class="section" id="migration-uris">
<h1><a class="toc-backref" href="#id10">Migration URIs</a><a class="headerlink" href="#migration-uris" title="Link to this headline">¶</a></h1>
<p>Initiating a guest migration requires the client application to specify up to
three URIs, depending on the choice of control flow and/or APIs used. The first
URI is that of the libvirt connection to the source host, where the virtual
guest is currently running. The second URI is that of the libvirt connection to
the destination host, where the virtual guest will be moved to (and in
peer-to-peer migrations, this is from the perspective of the source, not the
client). The third URI is a hypervisor specific URI used to control how the
guest will be migrated. With any managed migration flow, the first and second
URIs are compulsory, while the third URI is optional. With the unmanaged direct
migration mode, the first and third URIs are compulsory and the second URI is
not used.</p>
<p>Ordinarily management applications only need to care about the first and second
URIs, which are both in the normal libvirt connection URI format. Libvirt will
then automatically determine the hypervisor specific URI, by looking up the
target host's configured hostname. There are a few scenarios where the
management application may wish to have direct control over the third URI.</p>
<ol class="arabic simple">
<li><p>The configured hostname is incorrect, or DNS is broken. If a host has a
hostname which will not resolve to match one of its public IP addresses, then
libvirt will generate an incorrect URI. In this case the management
application should specify the hypervisor specific URI explicitly, using an
IP address, or a correct hostname.</p></li>
<li><p>The host has multiple network interfaces. If a host has multiple network
interfaces, it might be desirable for the migration data stream to be sent
over a specific interface for either security or performance reasons. In this
case the management application should specify the hypervisor specific URI,
using an IP address associated with the network to be used.</p></li>
<li><p>The firewall restricts what ports are available. When libvirt generates a
migration URI it will pick a port number using hypervisor specific rules.
Some hypervisors only require a single port to be open in the firewalls,
while others require a whole range of port numbers. In the latter case the
management application may wish to choose a specific port number outside the
default range in order to comply with local firewall policies.</p></li>
<li><p>The second URI uses UNIX transport method. In this advanced case libvirt
should not guess a *migrateuri* and it should be specified using UNIX
socket path URI: <span class="docutils literal"><span class="pre">unix:///path/to/socket</span></span>.</p></li>
</ol>
</div>
<div class="section" id="configuration-file-handling">
<h1><a class="toc-backref" href="#id11">Configuration file handling</a><a class="headerlink" href="#configuration-file-handling" title="Link to this headline">¶</a></h1>
<p>There are two types of virtual machines known to libvirt. A <em>transient</em> guest
only exists while it is running, and has no configuration file stored on disk. A
<em>persistent</em> guest maintains a configuration file on disk even when it is not
running.</p>
<p>By default, a migration operation will not attempt to modify any configuration
files that may be stored on either the source or destination host. It is the
administrator, or management application's, responsibility to manage
distribution of configuration files (if desired). It is important to note that
the <span class="docutils literal">/etc/libvirt</span> directory <strong>MUST NEVER BE SHARED BETWEEN HOSTS</strong>. There are
some typical scenarios that might be applicable:</p>
<ul class="simple">
<li><p>Centralized configuration files outside libvirt, in shared storage. A cluster
aware management application may maintain all the master guest configuration
files in a cluster filesystem. When attempting to start a guest, the config
will be read from the cluster FS and used to deploy a persistent guest. For
migration the configuration will need to be copied to the destination host
and removed on the original.</p></li>
<li><p>Centralized configuration files outside libvirt, in a database. A data center
management application may not store configuration files at all. Instead it
may generate libvirt XML on the fly when a guest is booted. It will typically
use transient guests, and thus not have to consider configuration files
during migration.</p></li>
<li><p>Distributed configuration inside libvirt. The configuration file for each
guest is copied to every host where the guest is able to run. Upon migration
the existing config merely needs to be updated with any changes.</p></li>
<li><p>Ad-hoc configuration management inside libvirt. Each guest is tied to a
specific host and rarely migrated. When migration is required, the config is
moved from one host to the other.</p></li>
</ul>
<p>As mentioned above, libvirt will not modify configuration files during migration
by default. The <span class="docutils literal">virsh</span> command has two flags to influence this behaviour. The
<span class="docutils literal"><span class="pre">--undefinesource</span></span> flag will cause the configuration file to be removed on the
source host after a successful migration. The <span class="docutils literal"><span class="pre">--persistent</span></span> flag will cause a
configuration file to be created on the destination host after a successful
migration. The following table summarizes the configuration file handling in all
possible state and flag combinations.</p>
<table>
<colgroup>
<col style="width: 13%"/>
<col style="width: 13%"/>
<col style="width: 13%"/>
<col style="width: 13%"/>
<col style="width: 13%"/>
<col style="width: 13%"/>
<col style="width: 13%"/>
<col style="width: 13%"/>
</colgroup>
<thead>
<tr><th class="head" colspan="3"><p>Before migration</p></th>
<th class="head" colspan="2"><p>Flags</p></th>
<th class="head" colspan="3"><p>After migration</p></th>
</tr>
<tr><th class="head"><p>Source type</p></th>
<th class="head"><p>Source config</p></th>
<th class="head"><p>Dest config</p></th>
<th class="head"><p>--undefinesource</p></th>
<th class="head"><p>--persistent</p></th>
<th class="head"><p>Dest type</p></th>
<th class="head"><p>Source config</p></th>
<th class="head"><p>Dest config</p></th>
</tr>
</thead>
<tbody>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y
(unchanged dest
config)</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y
(unchanged dest
config)</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y
(replaced with
source)</p></td>
</tr>
<tr><td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y
(replaced with
source)</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Transient</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Transient</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y
(unchanged dest
config)</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y
(unchanged dest
config)</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y
(replaced with
source)</p></td>
</tr>
<tr><td><p>Persistent</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Persistent</p></td>
<td><p>N</p></td>
<td><p>Y
(replaced with
source)</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="migration-scenarios">
<h1><a class="toc-backref" href="#id12">Migration scenarios</a><a class="headerlink" href="#migration-scenarios" title="Link to this headline">¶</a></h1>
<div class="section" id="native-migration-client-to-two-libvirtd-servers">
<h2><a class="toc-backref" href="#id13">Native migration, client to two libvirtd servers</a><a class="headerlink" href="#native-migration-client-to-two-libvirtd-servers" title="Link to this headline">¶</a></h2>
<p>At an API level this requires use of virDomainMigrate, without the
VIR_MIGRATE_PEER2PEER flag set. The destination libvirtd server will
automatically determine the native hypervisor URI for migration based off the
primary hostname. To force migration over an alternate network interface the
optional hypervisor specific URI must be provided</p>
<pre class="literal-block">syntax: virsh migrate GUESTNAME DEST-LIBVIRT-URI [HV-URI]


eg using default network interface

virsh migrate web1 qemu+ssh://desthost/system
virsh migrate web1 xen+tls://desthost/system


eg using secondary network interface

virsh migrate web1 qemu://desthost/system tcp://10.0.0.1/</pre>
<p>Supported by Xen, QEMU, VMware and VirtualBox drivers</p>
</div>
<div class="section" id="native-migration-client-to-and-peer2peer-between-two-libvirtd-servers">
<h2><a class="toc-backref" href="#id14">Native migration, client to and peer2peer between, two libvirtd servers</a><a class="headerlink" href="#native-migration-client-to-and-peer2peer-between-two-libvirtd-servers" title="Link to this headline">¶</a></h2>
<p>virDomainMigrate, with the VIR_MIGRATE_PEER2PEER flag set, using the libvirt URI
format for the 'uri' parameter. The destination libvirtd server will
automatically determine the native hypervisor URI for migration, based off the
primary hostname. The optional uri parameter controls how the source libvirtd
connects to the destination libvirtd, in case it is not accessible using the
same address that the client uses to connect to the destination, or a different
encryption/auth scheme is required. There is no scope for forcing an alternative
network interface for the native migration data with this method.</p>
<p>This mode cannot be invoked from virsh</p>
<p>Supported by QEMU driver</p>
</div>
<div class="section" id="tunnelled-migration-client-and-peer2peer-between-two-libvirtd-servers">
<h2><a class="toc-backref" href="#id15">Tunnelled migration, client and peer2peer between two libvirtd servers</a><a class="headerlink" href="#tunnelled-migration-client-and-peer2peer-between-two-libvirtd-servers" title="Link to this headline">¶</a></h2>
<p>virDomainMigrate, with the VIR_MIGRATE_PEER2PEER &amp; VIR_MIGRATE_TUNNELLED flags
set, using the libvirt URI format for the 'uri' parameter. The destination
libvirtd server will automatically determine the native hypervisor URI for
migration, based off the primary hostname. The optional uri parameter controls
how the source libvirtd connects to the destination libvirtd, in case it is not
accessible using the same address that the client uses to connect to the
destination, or a different encryption/auth scheme is required. The native
hypervisor URI format is not used at all.</p>
<p>This mode cannot be invoked from virsh</p>
<p>Supported by QEMU driver</p>
</div>
<div class="section" id="native-migration-client-to-one-libvirtd-server">
<h2><a class="toc-backref" href="#id16">Native migration, client to one libvirtd server</a><a class="headerlink" href="#native-migration-client-to-one-libvirtd-server" title="Link to this headline">¶</a></h2>
<p>virDomainMigrateToURI, without the VIR_MIGRATE_PEER2PEER flag set, using a
hypervisor specific URI format for the 'uri' parameter. There is no use or
requirement for a destination libvirtd instance at all. This is typically used
when the hypervisor has its own native management daemon available to handle
incoming migration attempts on the destination.</p>
<pre class="literal-block">syntax: virsh migrate GUESTNAME HV-URI


eg using same libvirt URI for all connections</pre>
</div>
<div class="section" id="native-migration-peer2peer-between-two-libvirtd-servers">
<h2><a class="toc-backref" href="#id17">Native migration, peer2peer between two libvirtd servers</a><a class="headerlink" href="#native-migration-peer2peer-between-two-libvirtd-servers" title="Link to this headline">¶</a></h2>
<p>virDomainMigrateToURI, with the VIR_MIGRATE_PEER2PEER flag set, using the
libvirt URI format for the 'uri' parameter. The destination libvirtd server will
automatically determine the native hypervisor URI for migration, based off the
primary hostname. There is no scope for forcing an alternative network interface
for the native migration data with this method. The destination URI must be
reachable using the source libvirtd credentials (which are not necessarily the
same as the credentials of the client in connecting to the source).</p>
<pre class="literal-block">syntax: virsh migrate GUESTNAME DEST-LIBVIRT-URI [ALT-DEST-LIBVIRT-URI]


eg using same libvirt URI for all connections

virsh migrate --p2p web1 qemu+ssh://desthost/system


eg using different libvirt URI auth scheme for peer2peer connections

virsh migrate --p2p web1 qemu+ssh://desthost/system qemu+tls:/desthost/system


eg using different libvirt URI hostname for peer2peer connections

virsh migrate --p2p web1 qemu+ssh://desthost/system qemu+ssh://10.0.0.1/system</pre>
<p>Supported by the QEMU driver</p>
</div>
<div class="section" id="tunnelled-migration-peer2peer-between-two-libvirtd-servers">
<h2><a class="toc-backref" href="#id18">Tunnelled migration, peer2peer between two libvirtd servers</a><a class="headerlink" href="#tunnelled-migration-peer2peer-between-two-libvirtd-servers" title="Link to this headline">¶</a></h2>
<p>virDomainMigrateToURI, with the VIR_MIGRATE_PEER2PEER &amp; VIR_MIGRATE_TUNNELLED
flags set, using the libvirt URI format for the 'uri' parameter. The destination
libvirtd server will automatically determine the native hypervisor URI for
migration, based off the primary hostname. The optional uri parameter controls
how the source libvirtd connects to the destination libvirtd, in case it is not
accessible using the same address that the client uses to connect to the
destination, or a different encryption/auth scheme is required. The native
hypervisor URI format is not used at all. The destination URI must be reachable
using the source libvirtd credentials (which are not necessarily the same as the
credentials of the client in connecting to the source).</p>
<pre class="literal-block">syntax: virsh migrate GUESTNAME DEST-LIBVIRT-URI [ALT-DEST-LIBVIRT-URI]


eg using same libvirt URI for all connections

virsh migrate --p2p --tunnelled web1 qemu+ssh://desthost/system


eg using different libvirt URI auth scheme for peer2peer connections

virsh migrate --p2p --tunnelled web1 qemu+ssh://desthost/system qemu+tls:/desthost/system


eg using different libvirt URI hostname for peer2peer connections

virsh migrate --p2p --tunnelled web1 qemu+ssh://desthost/system qemu+ssh://10.0.0.1/system</pre>
<p>Supported by QEMU driver</p>
</div>
<div class="section" id="migration-using-only-unix-sockets">
<h2><a class="toc-backref" href="#id19">Migration using only UNIX sockets</a><a class="headerlink" href="#migration-using-only-unix-sockets" title="Link to this headline">¶</a></h2>
<p>In niche scenarios where libvirt daemon does not have access to the network
(e.g. running in a restricted container on a host that has accessible network),
when a management application wants to have complete control over the transfer
or when migrating between two containers on the same host all the communication
can be done using UNIX sockets. This includes connecting to non-standard socket
path for the destination daemon, using UNIX sockets for hypervisor's
communication or for the NBD data transfer. All of that can be used with both
peer2peer and direct migration options.</p>
<p>Example using <span class="docutils literal">/tmp/migdir</span> as a directory representing the same path visible
from both libvirt daemons. That can be achieved by bind-mounting the same
directory to different containers running separate daemons or forwarding
connections to these sockets manually (using <span class="docutils literal">socat</span>, <span class="docutils literal">netcat</span> or a custom
piece of software):</p>
<pre class="literal-block">virsh migrate --domain web1 [--p2p] --copy-storage-all
  --desturi 'qemu+unix:///system?socket=/tmp/migdir/test-sock-driver'
  --migrateuri 'unix:///tmp/migdir/test-sock-qemu'
  --disks-uri unix:///tmp/migdir/test-sock-nbd</pre>
<p>One caveat is that on SELinux-enabled systems all the sockets that the
hypervisor is going to connect to needs to have the proper context and that is
chosen before its creation by the process that creates it. That is usually done
by using <span class="docutils literal"><span class="pre">setsockcreatecon{,raw}()</span></span> functions. Generally
*system_r:system_u:svirt_socket_t:s0* should do the trick, but check the
SELinux rules and settings of your system.</p>
<p>Supported by QEMU driver</p>
</div>
<div class="section" id="migration-of-vms-using-non-shared-images-for-disks">
<h2><a class="toc-backref" href="#id20">Migration of VMs using non-shared images for disks</a><a class="headerlink" href="#migration-of-vms-using-non-shared-images-for-disks" title="Link to this headline">¶</a></h2>
<p>Libvirt by default expects that the disk images which are not explicitly network
accessed are shared between the hosts by means of a network filesystem or remote
block storage.</p>
<p>By default it's expected that they are in the same location, but this can be
modified by providing an updated domain XML with appropriate paths to the images
using <span class="docutils literal"><span class="pre">--xml</span></span> argument for <span class="docutils literal">virsh migrate</span>.</p>
<p>In case when one or more of the images are residing on local storage libvirt
can migrate them as part of the migration flow. This is enabled using
<span class="docutils literal"><span class="pre">--copy-storage-all</span></span> flag for <span class="docutils literal">virsh migrate</span>. Additionally
<span class="docutils literal"><span class="pre">--migrate-disks</span></span> parameter allows control which disks need to actually be
migrated. Without the flag all read-write disks are migrated.</p>
<p>On the destination the images must be either pre-created by the user having
correct format and size or alternatively if the target path resides within a
libvirt storage pool they will be automatically created.</p>
<p>In case when the user wishes to migrate only the topmost image from a backing
chain of images for each disks <span class="docutils literal"><span class="pre">--copy-storage-inc</span></span> can be used instead. User
must pre-create the images unconditionally.</p>
<p>In order to ensure that the migration of disks will not be overwhelmed by a
guest doing a lot of I/O to a local fast storage the
<span class="docutils literal"><span class="pre">--copy-storage-synchronous-writes</span></span> flag ensures that newly written data is
synchronously written to the destination. This may harm I/O performance during
the migration.</p>
</div>
</div>
</div>
    </div>
    <div id="nav">
      <div id="home">
        <a href="index.html">Home</a>
      </div>
      <div id="jumplinks">
        <ul>
          <li>
            <a href="downloads.html">Download</a>
          </li>
          <li>
            <a href="contribute.html">Contribute</a>
          </li>
          <li>
            <a href="docs.html">Docs</a>
          </li>
        </ul>
      </div>
      <div id="search">
        <form id="simplesearch" action="https://www.google.com/search" enctype="application/x-www-form-urlencoded" method="get">
          <div>
            <input id="searchsite" name="sitesearch" type="hidden" value="libvirt.org"/>
            <input id="searchq" name="q" type="text" size="12" value=""/>
            <input name="submit" type="submit" value="Go"/>
          </div>
        </form>
        <div id="advancedsearch">
          <span>
            <input type="radio" name="what" id="whatwebsite" checked="checked" value="website"/>
            <label for="whatwebsite">Website</label>
          </span>
          <span>
            <input type="radio" name="what" id="whatwiki" value="wiki"/>
            <label for="whatwiki">Wiki</label>
          </span>
          <span>
            <input type="radio" name="what" id="whatdevs" value="devs"/>
            <label for="whatdevs">Developers list</label>
          </span>
          <span>
            <input type="radio" name="what" id="whatusers" value="users"/>
            <label for="whatusers">Users list</label>
          </span>
        </div>
      </div>
    </div>
    <div id="footer">
      <div id="contact">
        <h3>Contact</h3>
        <ul>
          <li>
            <a href="contact.html#mailing-lists">email</a>
          </li>
          <li>
            <a href="contact.html#irc">irc</a>
          </li>
        </ul>
      </div>
      <div id="community">
        <h3>Community</h3>
        <ul>
          <li>
            <a href="https://twitter.com/hashtag/libvirt">twitter</a>
          </li>
          <li>
            <a href="https://stackoverflow.com/questions/tagged/libvirt">stackoverflow</a>
          </li>
          <li>
            <a href="https://serverfault.com/questions/tagged/libvirt">serverfault</a>
          </li>
        </ul>
      </div>
      <div id="contribute">
        <h3>Contribute</h3>
        <ul>
          <li>
            <a href="https://gitlab.com/libvirt/libvirt/-/blob/master/docs/migration.rst">edit this page</a>
          </li>
        </ul>
      </div>
      <div id="conduct">
            Participants in the libvirt project agree to abide by <a href="governance.html#code-of-conduct">the project code of conduct</a></div>
      <br class="clear"/>
    </div>
  </body>
</html>
